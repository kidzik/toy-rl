{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendulum implementation\n",
    "import gym\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Elementary implementation of Deep Q-learning Agent\n",
    "# from https://keon.io/deep-q-learning/\n",
    "from dqn import DQNAgent\n",
    "\n",
    "# Implementation of a visualization of the Q function\n",
    "from helpers import plot_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 20\n",
    "\n",
    "# initialize gym environment and the agent\n",
    "env = gym.make('Pendulum-v0')\n",
    "nactions = 3\n",
    "agent = DQNAgent(2, nactions)\n",
    "episodes = 10000\n",
    "\n",
    "def simplify_pendulum_obs(obs):\n",
    "    return np.array([np.arccos(obs[0]) * np.sign(obs[1]), obs[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000000, score: -907.6615850044843, loss: 14.780041937556916\n",
      "episode: 5/1000000, score: -1069.8956949966423, loss: 54.87046575397562\n",
      "episode: 10/1000000, score: -744.7788432757548, loss: 11.037022604403319\n",
      "episode: 15/1000000, score: -1319.0793536228437, loss: 5.872732219802856\n",
      "episode: 20/1000000, score: -1181.2258767659475, loss: 69.37678545166273\n",
      "episode: 25/1000000, score: -1594.1169634103612, loss: 46.90653177897184\n",
      "episode: 30/1000000, score: -1395.6776969036046, loss: 94.25358753220644\n",
      "episode: 35/1000000, score: -1222.671427428283, loss: 3.178452866821317\n",
      "episode: 40/1000000, score: -1190.1301596348835, loss: 11.828887262148783\n",
      "episode: 45/1000000, score: -1617.5213815309462, loss: 9.000350598558725\n",
      "episode: 50/1000000, score: -1284.1187743820146, loss: 18.16108563677699\n",
      "episode: 55/1000000, score: -1170.243072492357, loss: 10.396541607825839\n",
      "episode: 60/1000000, score: -1284.8032817533544, loss: 12.182793427957222\n",
      "episode: 65/1000000, score: -856.0998557328621, loss: 8.834707143192645\n",
      "episode: 70/1000000, score: -970.7974815603172, loss: 231.1074209204089\n",
      "episode: 75/1000000, score: -1312.803912628578, loss: 8.362302222638391\n",
      "episode: 80/1000000, score: -1497.715819565434, loss: 5.576238568348344\n",
      "episode: 85/1000000, score: -1306.7124460973487, loss: 216.91713694885644\n",
      "episode: 90/1000000, score: -1154.6937101917526, loss: 10.221395878697876\n",
      "episode: 95/1000000, score: -856.3219422515433, loss: 11.9554649542697\n",
      "episode: 100/1000000, score: -1616.8569672730523, loss: 22.68205897253938\n",
      "episode: 105/1000000, score: -1051.426809467595, loss: 2.889305261458503\n",
      "episode: 110/1000000, score: -859.8234367746892, loss: 8.575711825978942\n",
      "episode: 115/1000000, score: -1133.0629325774232, loss: 42.467203371175856\n",
      "episode: 120/1000000, score: -1527.587044945358, loss: 45.92887167746812\n",
      "episode: 125/1000000, score: -1180.7477738140972, loss: 15.669844807125628\n",
      "episode: 130/1000000, score: -1385.864569498098, loss: 9.669453706563786\n",
      "episode: 135/1000000, score: -1517.218753961461, loss: 10.969813253483153\n",
      "episode: 140/1000000, score: -1333.2922213293916, loss: 145.73608881639666\n",
      "episode: 145/1000000, score: -1531.1002238174299, loss: 34.54107675066916\n",
      "episode: 150/1000000, score: -870.6727052407614, loss: 14.17621745003271\n",
      "episode: 155/1000000, score: -1502.2506436082263, loss: 136.84911182105134\n",
      "episode: 160/1000000, score: -1357.5678344969153, loss: 8.231159683782607\n",
      "episode: 165/1000000, score: -1462.2873229147758, loss: 3.0439418550768096\n",
      "episode: 170/1000000, score: -1320.839710473469, loss: 12.603945013430348\n",
      "episode: 175/1000000, score: -1498.015637927285, loss: 27.096954561401617\n",
      "episode: 180/1000000, score: -748.2436492609705, loss: 6.514876868576408\n",
      "episode: 185/1000000, score: -1630.5596087737897, loss: 9.285190716162106\n",
      "episode: 190/1000000, score: -1629.153279635416, loss: 8.784010743373074\n",
      "episode: 195/1000000, score: -1523.6016702306597, loss: 209.96438493582536\n",
      "episode: 200/1000000, score: -1313.3217757628486, loss: 79.58446137137798\n",
      "episode: 205/1000000, score: -1497.6636472261232, loss: 133.08082282950636\n",
      "episode: 210/1000000, score: -1442.7343679676594, loss: 2.553502419603319\n",
      "episode: 215/1000000, score: -1490.6085944521305, loss: 2.913695164010278\n",
      "episode: 220/1000000, score: -1205.7344351518138, loss: 55.22578905244927\n",
      "episode: 225/1000000, score: -1540.7955790815279, loss: 32.177669681449515\n",
      "episode: 230/1000000, score: -1373.062359405538, loss: 21.663473903556223\n",
      "episode: 235/1000000, score: -1255.2971511269466, loss: 7.205515246147115\n",
      "episode: 240/1000000, score: -1455.8621529750753, loss: 3.451346864167135\n",
      "episode: 245/1000000, score: -1346.0842987428389, loss: 45.98159836389823\n",
      "episode: 250/1000000, score: -1619.239966993943, loss: 10.091444430872798\n",
      "episode: 255/1000000, score: -1231.6485586829613, loss: 29.091865060403677\n",
      "episode: 260/1000000, score: -1192.7314814022584, loss: 18.930592056611204\n",
      "episode: 265/1000000, score: -1405.2273345175404, loss: 14.007124166935682\n",
      "episode: 270/1000000, score: -1312.5175595790429, loss: 9.474051021803461\n",
      "episode: 275/1000000, score: -1509.4482462538317, loss: 3.16611096550605\n",
      "episode: 280/1000000, score: -504.1619060043657, loss: 9.96662914755143\n",
      "episode: 285/1000000, score: -981.976184243798, loss: 6.470439822238404\n",
      "episode: 290/1000000, score: -1483.249462506638, loss: 15.455985135826268\n",
      "episode: 295/1000000, score: -1328.7127961076578, loss: 135.44423260961776\n",
      "episode: 300/1000000, score: -1460.2128641238128, loss: 3.533085750270402\n",
      "episode: 305/1000000, score: -1482.056004912937, loss: 8.504877966319327\n",
      "episode: 310/1000000, score: -1373.6566506667662, loss: 163.052226057589\n",
      "episode: 315/1000000, score: -873.1429358960403, loss: 26.721338753777673\n",
      "episode: 320/1000000, score: -1012.1027295446659, loss: 32.88752241502516\n",
      "episode: 325/1000000, score: -1738.8647987889574, loss: 3.6857426745264092\n",
      "episode: 330/1000000, score: -875.4374466378238, loss: 17.897154069531098\n",
      "episode: 335/1000000, score: -1070.4291409483733, loss: 9.097295206771378\n",
      "episode: 340/1000000, score: -1508.711336263236, loss: 3.677832811958069\n",
      "episode: 345/1000000, score: -1543.176719943585, loss: 4.111892446409911\n",
      "episode: 350/1000000, score: -968.3356317818921, loss: 2.236276007257402\n",
      "episode: 355/1000000, score: -1328.1925701924367, loss: 3.8801097471732646\n",
      "episode: 360/1000000, score: -961.602557514558, loss: 2.9771387212731497\n",
      "episode: 365/1000000, score: -1396.722543147148, loss: 12.607656352397157\n",
      "episode: 370/1000000, score: -882.1393138636063, loss: 25.44413646211615\n",
      "episode: 375/1000000, score: -1500.3389286485021, loss: 3.988270020861819\n",
      "episode: 380/1000000, score: -1522.175154078731, loss: 10.526812878824785\n",
      "episode: 385/1000000, score: -1148.4133138747816, loss: 2.9407646251347614\n",
      "episode: 390/1000000, score: -1138.1979157711885, loss: 8.118715514171527\n",
      "episode: 395/1000000, score: -1292.828789945176, loss: 2.5335211102938047\n",
      "episode: 400/1000000, score: -975.8480449282098, loss: 2.2145101203377635\n",
      "episode: 405/1000000, score: -1475.741634403277, loss: 4.887420383354765\n",
      "episode: 410/1000000, score: -991.1656936820151, loss: 2.8676748704465354\n",
      "episode: 415/1000000, score: -1024.725588109079, loss: 7.257624159912666\n",
      "episode: 420/1000000, score: -736.8877414421677, loss: 82.63851898603025\n",
      "episode: 425/1000000, score: -1503.7591998505193, loss: 16.190303885261528\n",
      "episode: 430/1000000, score: -920.5930259644387, loss: 1.9105557175789727\n",
      "episode: 435/1000000, score: -988.4029985376216, loss: 13.560004376925122\n",
      "episode: 440/1000000, score: -1215.3405774298444, loss: 24.835108221275732\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"frames\"):\n",
    "    os.mkdir(\"frames\")\n",
    "\n",
    "episodes = 1000000\n",
    "action_scale = (agent.action_size - 1)/2\n",
    "\n",
    "A = np.reshape(np.array([]),[0,2 ])\n",
    "\n",
    "# Iterate the game\n",
    "for e in range(episodes):\n",
    "    # reset state in the beginning of each game\n",
    "    state = env.reset()\n",
    "    state = simplify_pendulum_obs(state)\n",
    "    state = np.reshape(state, [1, 2])\n",
    "    \n",
    "    # time_t represents each frame of the game\n",
    "    # Our goal is to keep the pole upright as long as possible until score of 500\n",
    "    # the more time_t the more score\n",
    "    total = 0\n",
    "    for time_t in range(500):\n",
    "        # turn this on if you want to render\n",
    "        # env.render()\n",
    "        # Decide action\n",
    "        action = [agent.act(state)]\n",
    "        \n",
    "        # Advance the game to the next frame based on the action.\n",
    "        # Reward is 1 for every frame the pole survived\n",
    "        next_state, reward, done, _ = env.step( [ (action[0]-action_scale)*2.0/float(action_scale) ])\n",
    "        next_state = simplify_pendulum_obs(next_state)\n",
    "        next_state = np.reshape(next_state, [1, 2])\n",
    "        \n",
    "        # Remember the previous state, action, reward, and done\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        total += reward\n",
    "\n",
    "        # done becomes True when the game ends\n",
    "        # ex) The agent drops the pole\n",
    "#        if e % 500 == 0:\n",
    "#            env.render()\n",
    "        if done:\n",
    "            # print the score and break out of the loop\n",
    "            break\n",
    "            \n",
    "    # train the agent with the experience of the episode\n",
    "    loss = agent.replay(32)\n",
    "    A = np.vstack([A, [total, loss]])\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print(\"episode: {}/{}, score: {}, loss: {}\".format(e, episodes, total, loss))\n",
    "#        plot_q(agent, e/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
